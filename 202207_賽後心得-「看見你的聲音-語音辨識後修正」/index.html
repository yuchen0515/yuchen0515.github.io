<!DOCTYPE html><html lang="zh-TW"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>賽後心得-「看見你的聲音--語音辨識後修正」 | Owen Lin website</title><!-- Global site tag (gtag.js) - Google Analytics--><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-CJPQK1CYBB"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-CJPQK1CYBB');</script><script src="//cdnjs.cloudflare.com/ajax/libs/blueimp-md5/2.19.0/js/md5.min.js"></script><script>var config = {"hostname":"https://yuchen0515.github.io","root":"/","preload":true,"path":"search.json"}</script><script src="/js/gitalk.js"></script><script src="//unpkg.com/mermaid@8.13.5/dist/mermaid.min.js"></script><script>mermaid.initialize({
  startOnLoad: false
  , theme: 'dark'
});</script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
  menuSettings: {
    zoom: "None"
  },
  showMathMenu: false,
  jax: ["input/TeX","output/CommonHTML"],
  extensions: ["tex2jax.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js"],
    equationNumbers: {
      autoNumber: "AMS"
    }
  },
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]]
  }
});</script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="//unpkg.com/@highlightjs/cdn-assets@11.4.0/styles/atom-one-dark-reasonable.min.css"><link rel="stylesheet" href="//unpkg.com/element-ui@2.15.8/lib/theme-chalk/index.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 6.1.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Owen Lin website" type="application/atom+xml">
</head><body style="background-image:url(https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg);"><div id="cursor-container"><div id="cursor-outer"></div><div id="cursor-effect"></div></div><main><header><nav><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><ol class="navContent"><li class="navItem search-header"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></li><li class="navItem"><a href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem"><a href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a href="/links/"><span class="navItemTitle">Useful Links</span></a></li><li class="navItem"><a href="/about/"><span class="navItemTitle">About</span></a></li></ol></nav><div class="search-popup"><div id="search-result"></div></div></header><article><div id="post-bg"><div id="post-title"><h1>賽後心得-「看見你的聲音--語音辨識後修正」</h1><div id="post-info"><span>First Post:<span class="control"><time datetime="2022-07-29T00:00:00.000Z" id="date"> 2022-07-29</time></span></span><br><span>Last Update:<span class="control"><time datetime="2023-02-11T18:20:33.264Z" id="updated"> 2023-02-12</time></span></span><br><span>Word Count:<span class="control">&nbsp;5.3k</span></span><br><span>Read Time:<span class="control">&nbsp;18min</span></span></div></div><hr><div id="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>決定命運的，不是偶然而是選擇。—電視劇 金裝律師 (Suits)</p>
</blockquote>
<p>過往的失意，鬱結為一股不具名的悲傷，慢慢地，蛻變為養分。大學後我開始注重自己的選擇，追尋自己的意志，探索自己要的是什麼，結果是—迎來戲劇性的大學生涯。</p>
<p>雲霄飛車，一次次跌宕起伏，大學生涯亦然。突然發現自己喜愛資訊領域，進入後突然被抓去比賽，小有成就卻慘敗，進入一股低潮。莫名的，就站在舞台上，獲頒那夢寐以求的銅獎…替我曾經覺得遙不可及的奧匹資訊賽命題。</p>
<p>糊裡糊塗的倉促準備實習，進到我想進的公司 KKBOX。卻意外勾勒起落地又揚起的音符—「語音」與興致勃勃的我。於是隨性地訂下持續升學並研究語音辨識的目標。我想…很慶幸的，我並不是源自於對出社會的恐懼而升學，也並非背負家人的選擇而選擇，而是衷於我自己的想望，這是我最幸運的一件事。</p>
<p>去年十一月，推甄上台大資工所，並如願加入「多媒體資訊檢索實驗室」的語音組，此時的我像剛進入幼兒園，純真而好奇地探索每樣事物，懵懵懂懂地甚是純真，緩步地吸取新知。</p>
<p>我想，並沒有所謂的偶然，而是選擇決定了自己的命運吧！</p>
<h2 id="緣起"><a href="#緣起" class="headerlink" title="緣起"></a>緣起</h2><p>實習、實驗室和課程，如此稀疏平常的生活一天天過去了。某天，兩位涉世未深的「碩 0 生」被指派參加玉山銀行舉辦的「<a target="_blank" rel="noopener" href="https://tbrain.trendmicro.com.tw/Competitions/Details/23">看見你的聲音—語音辨識後修正</a>」競賽，沒錯…其中一位就是我！</p>
<hr>
<h2 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h2><p>我們參加的是玉山銀行主辦的「看見你的聲音—語音辨識後修正」，看起來倒像綠豆糕，痾…不是！看起來很像「語音辨識」的主題，不過這塊領域長期是乏人問津，投入該領域的人並不多。事實上，他是「後修正」，意即做完語音辨識後，以「自然語言處理 (Natural Language Processing, NLP)」手法再進一步修正。</p>
<p>NLP 領域其實也並不多人，因此該比賽在 T-brain 平台上是最少人參與的，即便如此，仍有一百一十九隊參加！</p>
<p>銀行業近年來盛行導入智慧 AI 機器人，而該場比賽情境也十分雷同。玉山 APP 提供智慧化的語音服務，服務接受使用者的聲音後，經過 ASR (Automatic Speech Recognition) 模型辨識，產生出前十可能對應的語句—這些都是前半段「語音辨識」所屬的範疇。</p>
<p>賽事所做的就是後半段的「語音後修正」，依據語音辨識的結果，可利用不同候選句之間的關聯再次糾正，即為本次賽事的主要任務，不過…當然不僅如此而已，比賽較像是要求我們提供「正式對外服務」，我們需要架設 API Server 對外提供服務…！</p>
<p>每筆 Request 即為可能語句列表、時間和重試次數等等，當服務接受需求後，必須在一秒內回傳一句結果。然而…要命的來了，同時可能有十筆 Requests 席捲而來！更何況是在日益肥大的 NLP 領域達成這樣的豐功偉業，也因此一度「人性化」地在賽中放寬至兩秒，但仍舊是個很浮誇的需求。</p>
<h2 id="小小不幸"><a href="#小小不幸" class="headerlink" title="小小不幸"></a>小小不幸</h2><p>何謂不幸？身為涉世未深的碩 0 學生，在賽事開始後一個月才參賽，以一個狀況外的鄉巴佬姿態誕生，如同新手逛街打怪，遇見地獄大魔王一般。我與隊友更是第一次參加 AI 相關賽事，憑藉隊友曾是 NLP 的過客，配合我在實習鍛鍊的後端能力，組成一支全面性俱佳的隊伍—CUDAOutOfMemory。</p>
<p>敝隊駕馭的電腦配備兩張 GeForce GTX 1080 Ti，而在我們初次與其相見歡的剎那，Out Of Memory 纏身—就那樣…戲謔性地成為了隊名XD</p>
<hr>
<h2 id="NLP-Model"><a href="#NLP-Model" class="headerlink" title="NLP Model"></a>NLP Model</h2><p>綜觀比賽本身，可以注意到不僅後端需求相當高，以語句本身的 baseline 亦是相當傑出，約略為 9.34 % CER (Character Error Rate)，這是經過玉山 ASR 辨識後的最佳結果。奠定在這基礎上，還想要更好的表現實屬不易，還得承受高強度的需求，只能說獲獎希望渺茫，藍瘦香菇啊……</p>
<p>我們選擇肥碩的 Transformers model，將「糾錯問題視為一種翻譯任務」展開一系列操作，添加額外文本  <a target="_blank" rel="noopener" href="https://www.openslr.org/55/">CLMAD</a>、產生錯誤文本、困惑度 (Perplexity, PPL) 和 <a target="_blank" rel="noopener" href="https://github.com/baojunshan/nlp-fluency">nlp-fluency</a>。不幸地，在測試賽效果都十分有限。</p>
<p>零零總總的各方嘗試，於最終正式賽改進了 1 % 的 CER，成為唯一的 100% 使用 Supervised Learning 仍能獲獎的隊伍，真是萬幸啊XD</p>
<hr>
<h2 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h2><p>而我在這場比賽中，一肩扛起後端的重責。將這樣一個重大任務，交給一位初出茅廬，或許談不上是「後端工程師」的我來主掌，這隊也沒人才了…命在旦夕，得獎希望黯淡無光且渺茫啊…！</p>
<p>悵然若失、眼神空洞的….實驗室主機，一聲長嘆，然後一操再操。身為前六名中少數「超在意」後端的隊伍，也是少數從後端層面一次次加速，而達到賽事的要求，這也算是對實習一年來的成果驗收吧！</p>
<p>而接下來，就要面對一波波狀況，見招拆招了 :P</p>
<h3 id="狀況一"><a href="#狀況一" class="headerlink" title="狀況一"></a>狀況一</h3><p>想像一下，你在玩 Random Dice 這款骰子塔防遊戲。</p>
<p><img src="/images/pasted-1.png" alt="upload successful"></p>
<p>身為左邊的玩家，你可愛的骰子們等級都太嫩，一隻敵人就應付不暇，更何況後面一條龍。是的，由於 NLP Model — Transformers 的肥大，依據推論高達 2300 ms，遠超過主辦方要求，一個應付不暇，更何況一次來十個？</p>
<p>那麼，就得先針對問題本身思考，盡可能去蕪存菁。候選句全部都很重要嗎？這是第一個問題。人眼掃描後，可以發現越後面的語句越荒謬，在應付不暇的狀況下，其實第一句就已經堪用，保留了許多最重要的資訊了。</p>
<p>接下來，就得思考「模型」真有需要那麼「深度」嗎？由於訓練資料寡寡可數，大概六萬筆稱不上是大資料，Model level 太深，或許還有過度擬合 (Overfitting) 的風險，故可將層級壓低些。</p>
<p>如此一來，服務進展到單筆 700 ms 的狀態，可說是一大里程碑。但你可要知道「一個打十個」是多麽剽悍的事實。</p>
<h3 id="狀況二"><a href="#狀況二" class="headerlink" title="狀況二"></a>狀況二</h3><p>面對十個敵人，全數解決僅花一秒鐘。顯然地，隨著敵人增加，打倒全部所需的時間勢必也會線性成長，這對於服務本身也是相同的狀況。</p>
<p>因此，可從幾個層面著手：</p>
<ul>
<li>排程—Queue vs. Concurrent</li>
<li>設備—CPU vs. GPU</li>
<li>拒絕滿分—「想守住越多，失去的越多」</li>
</ul>
<p>拒絕滿分這項，其實離我們都很近。上了高中，若抱持著題題斃命的心態，數學就會使你一槍斃命。因此在以某種策略挑選題目一個個解出，才能達到最佳表現。而在本次賽事中，已知語句列表是按照排行榜排序的，因此若無暇處理它，那就回傳最佳的語句，就是一種損失最少的方式。</p>
<p>當前處理程序的方式屬於 佇列 (Queue) 排隊式的，程序其實和交通很相似，雪隧一但遭遇事故，整條國五就會呈現紫色地獄。如果有多條道路疏通，那就太好了！在處理要求 (Request) 就會面臨類似的窘境，但若以 Concurrent 的方式，則會讓整體服務速率都降低，若整體速率高於一秒，就更淒慘了…一筆也完成不了。</p>
<p>以 Concurrent 方式處理要求，我們實驗了 WSGI, Nginx 等正式部署工具，最終以 Flask 內建的 Thread 效能最佳，與其他兩項工具效能相差近三成。</p>
<p>Flask Thread 事實上是利用「快速切換」，在多筆 Request 間流轉、輪替，達成近似於「同時」處理的效果，而非真正意義上的「同時」，這是 Python 本身語言上的限制所致。總而言之，暫時以這樣的方式，配合 奴役 GPU 1080 和這三大策略，順利獲得以下的回應效能。</p>
<p>拒絕滿分這項，其實離我們都很近。上了高中，若抱持著題題斃命的心態，數學就會使你一槍斃命。因此在以某種策略挑選題目一個個解出，才能達到最佳表現。而在本次賽事中，已知語句列表是按照排行榜排序的，因此若無暇處理它，那就回傳最佳的語句，就是一種損失最少的方式。</p>
<p><img src="/images/pasted-3.png" alt="upload successful"><br><em><center>Figure. 第一次昇華的 Response Time</center></em></p>
<p>Response Time Graph 會記載「何時」收到的 Request，每筆 Request 間隔多久才回傳結果，可以很直觀地看到整體服務的狀況是否在時間內。很顯然地在上圖中隱隱約約不妙、悲劇溢於言表，多筆 Request 回應時間高於兩秒，這代表整體效能還是不夠快。</p>
<h3 id="狀況三—Batch"><a href="#狀況三—Batch" class="headerlink" title="狀況三—Batch"></a>狀況三—Batch</h3><p>同時處理大量需求，始終會因為「高速」輪替而犧牲掉變換的時間 (context switch)，換個角度思考，是不是可以設定一個時間，在這段時間內盡可能搜集 request，時間到時一口氣同時處理，那麼配合 GPU 平行處理的威力，肯定能提高效能的吧！？</p>
<p>結果找到了這項工具 <a target="_blank" rel="noopener" href="https://github.com/ShannonAI/service-streamer">ShannonAI/service-streamer</a> 完成了這項豐功偉業，Throughput 5.7 / sec 提升至 14.5 / sec，效率為原本的 254 % 之多。啊…斯巴拉西 (素晴らしい)！！<br><img src="/images/pasted-5.png" alt="upload successful"><br><em><center>Figure. 脫胎換骨的 RTG 結果</center></em></p>
<h3 id="測試"><a href="#測試" class="headerlink" title="測試"></a>測試</h3><p>速度到位了，就得進到測試，這是個博大精深的學問…</p>
<p>而我們用了三種方式一一去確認我們服務的運作、速率，以及在高壓狀態下是否能照常服務：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://postman.com">postman</a> 使我們能檢測 API 服務是否正常運作</li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/grequests/">grequest</a> 這項 Python 套件則讓人可對同時傳送 request 客製化</li>
<li><a target="_blank" rel="noopener" href="https://jmeter.apache.org">JMeter</a> 模擬十名使用者輪替傳送 request</li>
</ul>
<p>一項產品上線後能否正常維運，壓力測試是很重要的環節，而你要盡可能的「高壓」去欺凌你自己的服務，把自己當成「壞壞使用者」去設計一些奧客才會做的事在測試裡頭，長句、錯誤句等等，若服務能夠安然無恙的照常運作與輸出，那服務「可能」就沒問題了。</p>
<p><img src="/images/pasted-7.png" alt="upload successful"><br><em><center>Figure. Stress testing</center></em></p>
<p>上圖為 JMeter 的測試曲線圖，這套工具是提供測試的開源軟體，能夠模擬多名使用者運用服務的承載狀況。而從圖中能注意到紅、綠和藍色等等不同的曲線：</p>
<ul>
<li>流通量 (Throughput)：每分鐘能處理的需求 (Request) 量。</li>
<li>平均時間 (Average)：每筆需求處理的平均時間 (毫秒)。</li>
<li>中位時間 (Median)：所有樣本 (sample) 按照處理時間排序後之中位數。</li>
</ul>
<p>另外，可以設定「暖機」時間，在一個現實運作的服務中，流量也是逐步的上升與下降。我們可以仿照這樣的情境，設定幾秒後才達到最大用戶數，也因此綠色的流通量曲線並不是起初就維持高輸出，而是配合暖機時間逐步上升的。當然，JMeter 作為一個專業的測試工具，還有許多擬真情境的功能可以嘗試！這邊就不一一說明了。</p>
<p>總之！從這張圖中透露出服務能夠穩定的輸出某個定值，而單就當前的需求量而言，並不會影響到整體服務的效能，故能夠判定「可承受」比賽規格的服務需求。</p>
<h3 id="架構圖"><a href="#架構圖" class="headerlink" title="架構圖"></a>架構圖</h3><p>總之！從這張圖中透露出服務能夠穩定的輸出某個定值，而單就當前的需求量而言，並不會影響到整體服務的效能，故能夠判定「可承受」比賽規格的服務需求。</p>
<p><img src="/images/pasted-9.png" alt="upload successful"><br><em><center>Figure: API Server 架構圖</center></em></p>
<p>關於本隊的 API Server 是時候該做個總結了，以上是本隊的系統架構圖，流程大致如下：</p>
<ol>
<li>玉山發出 POST Request</li>
<li>Flask Server 為每個 Request 各創建一個 Thread 處理之</li>
<li>蒐集 150 毫秒以內的 Request，最大容量為 64 筆</li>
<li>每 150 毫秒 或超出容量上限，即 批次處理 (Batch)</li>
<li>對於每筆 Request，檢查與判定，若符合規格就跑 Model，否則直接吐回 TOP1 的語句內容。這邊回傳給 Request 對應的 Thread</li>
<li>回傳結果給玉山的伺服器</li>
</ol>
<h3 id="賽後檢討"><a href="#賽後檢討" class="headerlink" title="賽後檢討"></a>賽後檢討</h3><p>流程大致上並無問題，也蠻順暢的。事後重新省視賽況和程式碼，認為以下幾點仍可改良：</p>
<ul>
<li>nvidia-docker： 運作時，有一段流程會不斷地查詢網路上的特定資源，以致於效率拖沓。因此，可以追蹤整體執行流程，將那些資源送進本機端維護並且定期更新，以本機端形式查閱，能提高約略三成的效能。</li>
<li>multi-GPUs：由微軟公開的 <a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO &amp; DeepSpeed</a> 加速技術，能夠將 NLP 模型龐大的參數攤分在若干個 GPU 上，能夠有效提升效能。</li>
<li>Redis：是一種 Key-Value 的資料儲存形式，能夠高速查詢資料。因此我們能將超時的 Request 計算結果與唯一的任務代碼對應，那麼若再次重新發送就能夠接力計算，節省已逝去的時間。而本次使用的 service-streamer 工具中有提供 Redis 以 batch 方式加速的功能。</li>
<li>Processes：常言「雞蛋不能放在同個籃子裡」，若以單個 Process 組織整套系統，當該筆 Process 出了狀況，將葬送整個服務。而以本次系統架構而論，即為單筆 Process 再分割出若干個 Thread 執行不同任務，因此日後得多在 multi-process 和 排程這塊琢磨，並需更加留意「例外」的處理形式。</li>
<li>程式碼維護：本次設定檔以及參數等工具不夠全面，只做了一半，如 Config 檔在 api 程式碼中就沒有使用，ArgumentParser 參數工具反倒是常更改的參數沒有寫進去。另一方面，由於程式碼撰寫頗為急促，故無暇顧及 “clean code”，這些都是未來可以改進的方向。</li>
</ul>
<hr>
<h2 id="插曲——大災難"><a href="#插曲——大災難" class="headerlink" title="插曲——大災難"></a>插曲——大災難</h2><p>競賽有若干個階段，亦有兩梯次的測試賽以及正式賽。測試賽為期三天，在指定期間內每日晚間六點，準時轟炸你各位參賽者，一共四千題，完全比照正式賽之規格。而正式賽與測試賽唯一不同的在於連續日期從三天變為四天。</p>
<p>自詡為奴隸 (也許他並沒這樣說XD) 的實驗室電腦，很僥倖地在兩次測試賽長達六天內並未出任何狀況，盡心盡力地為我們付出，並穩定地繳交出兩萬四千題的答案。</p>
<p>然而同學間戲弄改編的名言：「關鍵時刻比衰小！」總是剎那間恍惚一瞬發生…正式賽第三天—我最忙碌的一天…發生了事故。當天早上，實驗室電腦們接一連二地發生狀況，或許是被攻擊，我們家的是自行關機，但這一重開機 CUDA 就出問題了，使得 GPU 1080 Ti 絲毫無用武之地…</p>
<p>左右觀察法—每個人都知曉的祕技，據聞是在考試時左看看、右看看就能獲取答案。而我使出了忍術「上下觀察法」，非常單純，如白雪般皎潔無瑕。僅僅是觀察位於自己前後兩支隊伍，前面 (第五名) 差距不小，而後面 (第七名) 也有著不容忽視的鴻溝，權衡之下，假設第三、第四天已 “baseline” 成績作結，綜合前兩天平均可以穩妥地取得第六名，那麼維持 “baseline” 採取最安全穩定的做法就萬事休矣了！</p>
<p>不久，迅速完成一個「無腦」服務，能夠在連線瞬間將 Request 內容列表第一句話吐回去，啊！真是完美！最終憑藉這樣穩定的服務度過比賽的下半餘生，並達成了領獎資格要求「低於 baseliine」之成績，以第六名作收，可喜可賀。</p>
<p>其實從這次事件中，可以學到兩件事：</p>
<blockquote>
<p>一、不要太有自信</p>
</blockquote>
<p>切忌，勿對服務的穩定性太有信心，必須戒慎恐懼好好關切他！本地端終究是有風險，除了遇到意外而終止程序外，當然還有「不可抗力」類型的機器罷工！上雲與地端都有各自的好處，但若能發展「監控」服務主動通知，並且有緊急應對的方式，才能應對服務的各種狀況。</p>
<blockquote>
<p>二、「爛」方法其實很好</p>
</blockquote>
<p>“Baseline” 什麼都不做—站在比賽以及服務的立場上，是相當切實的手法。比賽力求成效，而在已知能夠穩定獲取名次的狀況下，穩，再求精進才是上策，而勿追求卓越而功虧一簣，十分可惜。</p>
<p>服務依然以穩定為優先，而若全掛相當於 P0 事件，盡可能讓損失降低才是上策，因此若能夠緊急恢復出一個堪用版本，就幾乎只寫了一大半。能夠在緊急且資源有限的狀況下，盡可能地權衡局勢，致力於降低損失，會是提供服務時最重要的事情。</p>
<h2 id="如何做得更好"><a href="#如何做得更好" class="headerlink" title="如何做得更好"></a>如何做得更好</h2><p>這是我在賽後不斷思索的一件事，而從不同隊伍的做法中，領悟到幾件事情…</p>
<blockquote>
<p>一、看清資料的本質</p>
</blockquote>
<p>從官方給的六萬筆訓練資料中，若仔細觀察可以注意到有不少類似於新聞標題的語句：「行政院擬發放五倍券」，但也有很生活化的語句：「我想進行匯兌服務」。因此資料本身可以分成兩種取向：一般、新聞，並且很顯然的糾正這兩種不同類型的語句，應無法以相同的邏輯去糾正他。</p>
<p>除了語料源頭的分類以外，從測試資料本身亦可以做出不少變化。如同：「近年來股市迭宕不停」這樣的語句，從相似詞上看，「股市」或許容易被辨認為「不是」，每個語句會有多種相似詞彙，經過語音辨識後所辨識之語句，最有可能是「音對字不對」的狀況。</p>
<p>因此，我們可以建立「相似字詞典」，將單筆測資利用相似字詞典排列組合，塑造出更多測試資料。合理地推估六萬筆資料可擴增五倍、六倍以上，用以訓練模型的話，更可再進一步下降 1% 上下的 CER。</p>
<blockquote>
<p>二、快，還要更快！</p>
</blockquote>
<p>有很多加速技巧是沒碰過、聽過就不會知曉的，例如：微軟公開的 <a target="_blank" rel="noopener" href="https://onnxruntime.ai">ONNX Runtime</a> 是以 ONNX 格式儲存，能以其技術加速五倍以上的推理速度。亦或是攤分參數的技術，一樣是微軟開發的 ZeRO 以及 DeepSpeed，能將現有資源最大的活化。若能夠再配合本隊本次使用的 Batch 技術，那麼甚至可以一秒內同時應付二十筆 Requests 是很可能的。</p>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>本次賽事我們兩位涉世未深的 碩 0 好夥伴費盡了心力，用了許多技術，難能可貴的能在第一次參與這樣的比賽，就搶到很前面的名次，獲得獎項，這也很超乎我們的預期。</p>
<p>不僅如此，更驗證了過去一年實習絕非虛度，技術上有深度進展，軟實力亦是。奠定在此基礎上，熟悉了整個系統維運與加速的程序，也學習到更多技巧可以運用，相信下次會更得心應手！</p>
<p>這次比較可惜的就是整體上過於倉促，沒有看透資料本身，能達到的成效就會有所囿限，加速上也有很多手法可以併用，希望下次能有類似的比賽能夠參與，也期許未來能提供更穩定且快速的系統，在四處都留下我們的蹤跡吧！</p>
<hr>
<p><center>&lt;以下是本次賽事的程式碼以及影片&gt;</center></p>
<ul>
<li>Repo: <a target="_blank" rel="noopener" href="https://github.com/yuchen0515/2022-Competition-CUDAOutOfMemory">https://github.com/yuchen0515/2022-Competition-CUDAOutOfMemory</a></li>
<li>Youtube: <a target="_blank" rel="noopener" href="https://youtu.be/l-A8QkgyIxo">https://youtu.be/l-A8QkgyIxo</a></li>
<li>原文：<a target="_blank" rel="noopener" href="https://medium.com/@mathlin/賽後心得-看見你的聲音-語音辨識後修正-f6d147ff9950">https://medium.com/@mathlin/賽後心得-看見你的聲音-語音辨識後修正-f6d147ff9950</a></li>
</ul>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div id="footer-link" style="right: calc(50% - 1px);order: 1;border-right: 1px solid #fe2;padding-left: unset;max-width: calc(50% - 4px);"><a href="/202209_MIRlab-owen-lin-RD-page/">← Next MIRlab - owen.lin RD page</a></div></div></div><div id="comments"></div></div><div id="bottom-btn"><a id="to-top" onClick="index.scrolltop();" title="to top" style="opacity: 0; display: none;">∧</a></div></article><div class="aside-box"><aside><div id="aside-top"><div id="about"><a href="/" id="logo"><img src="/img/logo.png" alt="Logo"></a><h1 id="Dr"><a href="/">Owen Lin 林育辰</a></h1><div id="description"><p></p></div><!--if page.published === undefined--><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">2</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">7</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">2</span></div></section></div><div id="aside-block"><div id="toc-div"><h1>INDEX</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B7%A3%E8%B5%B7"><span class="toc-text">緣起</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%B4%B9"><span class="toc-text">介紹</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E5%B0%8F%E4%B8%8D%E5%B9%B8"><span class="toc-text">小小不幸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP-Model"><span class="toc-text">NLP Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#API-Server"><span class="toc-text">API Server</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%80%E6%B3%81%E4%B8%80"><span class="toc-text">狀況一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%80%E6%B3%81%E4%BA%8C"><span class="toc-text">狀況二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%80%E6%B3%81%E4%B8%89%E2%80%94Batch"><span class="toc-text">狀況三—Batch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B8%AC%E8%A9%A6"><span class="toc-text">測試</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%A7%8B%E5%9C%96"><span class="toc-text">架構圖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B3%BD%E5%BE%8C%E6%AA%A2%E8%A8%8E"><span class="toc-text">賽後檢討</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%92%E6%9B%B2%E2%80%94%E2%80%94%E5%A4%A7%E7%81%BD%E9%9B%A3"><span class="toc-text">插曲——大災難</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD"><span class="toc-text">如何做得更好</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B5%90%E8%AA%9E"><span class="toc-text">結語</span></a></li></ol></div></div></div><footer><nobr><span class="text-title">©</span><span class="text-content">Copyright. 2023</span></nobr><br><text>published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></text><text> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknight</a></text><wbr><text>by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></text></footer></aside></div></main><script src="/js/search.js"></script><script>const reset=_=>{mermaid.init(undefined, ('.mermaid')); gitalk = new Gitalk({
   clientID: 'c2f497e464ecdbd6a6d9',
   clientSecret: '999944935e84cbe04c85ea48e2444f63fa8aa6bb',
   repo: 'yuchen0515.github.io',
   owner: 'yuchen0515',
   admin: ['yuchen0515'],
   id: md5(location.pathname)
 })
 gitalk.render("comments")}</script><script src="//unpkg.com/@highlightjs/cdn-assets@11.4.0/highlight.min.js"></script><script src="/js/arknights.js"></script><script>document.addEventListener("load",reset())</script></body></html>